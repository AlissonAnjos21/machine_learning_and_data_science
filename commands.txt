 pd.read_csv((base de dados)) -> Recebe o caminho (sistema de pastas) de um arquivo csv e lê ele possibilitando, assim, a sua manipulação;

.head() -> Exibe os 5 primeiros valores da base de dados utilizada, esse número pode ser alterado ao se informar um parâmetro;

.tail() -> Exibe os 5 últimos valores da base de dados utilizada, esse número pode ser alterado ao se informar um parâmetro;

.describe() -> Exibe algumas coisas úteis, como número de respostas, médias, desvios padrão, etc;

np.unique(base de dados[coluna]) -> Recebe como parâmetro uma coluna da base de dados e retorna a as diferentes respostas presentes nas mesmas, ou seja, não repete valores iguais. Ao passar-se como parâmetro return_counts=True, ele informa a quantidade de vezes que aquele dado apareceu;

sns.countplot(x=(valor)) -> Recebe como parâmetro um valor para x e assim controi uma gráfico de barras mostrando a quantidade de cada uma das possibilidades desse valor x;

plt.show() -> Exibe o gráfico criado para o usuário através de uma interface gráfica;

plt.hist(x = base de dados[coluna]) -> Recebe um valor de x, com uma coluna da base de dados, criando com ele um gráfico de barras com intervalos;

px.scatter_matrix(base de dados, dimensions=[coluna1, coluna2, etc]) -> Recebe como parâmetros a base de dados e as colunas que serão exibidas e produz um gráfico para cada coluna com os campos tanto na direção vertical quanto na horizontal. Possui como um dos parâmetros opcionais o color, que recebe uma das colunas e, assim, exibe quais itens do gráfico correspondem aos intervalos do item da coluna informada;

.loc[base de dados[coluna] (condição se tiver)] -> Localiza os dados de alguma coluna; podendo ser também aplicado algum filtro;

.drop() -> Recebe como parâmetro o índice de um elemento que cumpre determianda condição, sendo que, os elementos que cumprirem tal condição serão apagados da base de dados;

.index -> Informa o índice de algo;

.sum -> Retorna a soma de determinada coisa. Dependendo de como utilizada, pode retornar quantas vezes algo apareceu;

.mean() -> Informa a média de algo;

.isnull() -> Informa se valores da tabela que são ou não nulos retornando True. Caso seja passado como parâmetro uma base de dados com determinada coluna, ele retorna True para os valores nulos dentro daquela coluna;

.fillna() -> Recebe um valor como parãmetro e preenche os campos com valores nulos. Para que ele consiga alterar o valor da variável da base de dados e salvar o novo valor nela, é necessário informar que o parâmetro inplace=True;

.isin -> Informa True caso o valor ao qual ele esteja acessando exista dentro de um intervalo que foi informado como parâmetro. Na maior parte das vezes usa-se uma lista contendo os valores desejados e informa-se como parâmetro;

.values -> Informa o valor de determinado item, convertendo-o para um numpy array;

.iloc -> Seleciona linhas e colunas do data frame, sendo que, o primeiro parâmetro é relativo as linhas desejadas. Já o segundo parâmetro, é referente às colunas a serem pegadas. Para selecioná-las, é necessário informar os intervalos, de forma semelhante a uma lista/tupla;

.min() -> Seleciona o menor valor de um agrupamento de valores;

.max() -> Seleciona o maior valor de um agrupamento de valores;

(objeto StandardScaler).fit_transform() -> Recebe como parâmetro uma lista de dados e os padroniza;

px.treemap(base de dados, path=[colunas]) -> Recebe a base de dados e a coluna desejada. Assim, criando uma representação retangularcom base nas respostas. Aquelas respostas que aparecem mais ficam maiores, equanto as que aparecem menos ficam proporcionalmente menores. Quando é informado mais de uma coluna, aparecem outras representações retangulares dentro da representação principal (primeira representação), esses sub-retângulos, assim como o retângulo principal, também são proporcionais ao número de ocorrencias;

px.parallel_categories(base de dados, dimensions=[coluna1, coluna2, etc]) -> Recebe como parâmetros a base de dados e as colunas que serão exibidas e gera uma representação gráfica que liga as ocorrencias (valores das colunas) uma com a outra por meio de linhas;

.columns -> Informa as colunas presentes na base de dados;

(objeto LabelEncoder).fit_transform(x[:, indice coluna]) -> Transforma um dado do tipo Categórico em um dado do tipo Numérico;

ColumnTransformer(transformers=[('OneHot', OneHotEncoder(), [lista com os índices das colunas que serão sub-divididas])], remainder='passthrough') -> Recebe uma lista com os valores dos índices que se deseja sub-dividir e os sub-divide, ou seja, para cada diferente opção de valor que uma coluna pode ter, ele cria uma sub-coluna, se o valor da linha corresponder ao da sub-coluna o valor dessa sub-coluna é 1, caso o valor da linha não corresponda ao da sub-coluna, o seu valor é 0. Só é possível ter um valor de 1 por cada linha de sub-coluna de coluna. Além disso, ao final, é necessário informar que o ramainder='passthrough', isso faz com que os índices das colunas da base de dados não selecionados continuem os mesmos. Caso contrário, essas colunas não selecionadas seriam pagadas, permecendo apenas aquelas selecionadas;

(objeto OneHotEncoder).fit_transform(x).toarray() -> Executa o processo citado anteriormente e converte o dado obtido para o formato array;

.toarray() -> Converte algo para o formato de array, tem coisas que não são capazes de serem transformadas em array;

.shape -> Retorna a quantidade de, respectivamente, linhas e colunas de uma base de dados;

(X_treinamento, X_teste, Y_treinamento, Y_teste) = train_test_split(x, y, test_size=(percentual de dados separados para o teste), random_state = (qualquer número (0 por convenção))) -> Separa dados para as variáveis de treinamento e para as variáveis de teste. O text_size é referente ao percentual de dados que você quer separar para o teste, ex: 0.25 = 25%. O random_state é para que após cada execução ele mantenha um valor padrão para os dados. Caso não fosse utilizado, a cada execução os valores seriam diferentes, por isso, é mais fácil utilizar esse atributo, principalmente em momentos de criação/teste. O número que ele recebe pode ser qualquer um, porém, por convenção, usá-se 0;

pickle.dump([variáveis], arquivo aberto) -> Possibilita salvar certas variáveis em arquivos. A partir disso, não é necessário realizar o pre-processamento de dados novamente em cada novo arquivo;

pickle.load(arquivo aberto) -> Carrega um arquivo pickle que contém os dados salvos, como mencionados no exemplo anterior;

.fit(x treinamento, y treinamento) -> Recebe a base de dados histórica para conseguir realizar os cálculos de predição;

.predict(listas contendo novos valores x) -> Recebe uma lista contendo uma ou mais listas com novos valores x. Com base nos dados históricos, ele calculará qual a chance do valor pertencer a uma das classes anteriormente informadas no método fit();

(objeto GaussianNB).classes_ -> Informa as classes existentes;

(objeto GaussianNB).class_count_ -> Informa quantas vezes cada classe aparece na base de dados;

(objeto GaussianNB).class_prior_ -> Informa a frequência em que cada classe aparece;

accuracy_score(y teste, valor obtido por predição) -> Retornará o percentual de acerto;

confusion_matrix(y_teste, valor obtido por predição) -> Retornará uma matriz, onde na primeira linha e na primeira coluna está os valores que eram e ele apontou que eram, na primeira linha e segunda coluna está os valores que não eram e ele apontou como se fossem, na segunda linha e primeira coluna estão os valores que não eram e ele apontou que eram, por fim, na segunda linha e segunda coluna estão os valores que não eram e ele apontou como se não fossem;

(objeto ConfusionMatrix).score(x teste, y teste) -> Após receber o x teste ele realiza os cálculos da técnica Naive Bayes e após isso compara com o resultado oficial, ou seja, com o y teste;

classification_report(y teste, valor obtido por predicao) -> Informa alguns dados interessantes sobre os acertos e os erros da predição;

DecisionTreeClassifier(criterion='entropy', random_state = (qualquer número)) -> Permite instanciar um objeto para que a partir deste seja criada uma árvore de decisão baseada no ganho da entropia. O random_state, como já mencionado anteriormente, é útil para que após cada execução os resultados encontrados sejam os mesmos;

(objeto DecisionTreeClassifier).feature_importances_ -> Informa a importância de cada um dos atributos com base no ganho de informação por entropia;

tree.plot_tree(objeto DecisionTreeClassifier, feature_names=(variável com uma lista contendo os atributos), class_names = (objeto DecisionTreeClassifier).classes_, filled = True) -> Informa os ramos da árvore e seus respectivos ganhos de informação com base na entropia. O Parâmetro feature_names auxilia na compreenção do atributo ao qual o galho se refere, o class_names auxilia na compreenção da classe a qual o galho se refere, e o filled = True melhora a representação gráfica da árvore obtida, colorindo-a;

RandomForestClassifier(n_estimators=(número de árvores que você quer), criterion='entropy', random_state=(qualquer número)) -> Cria um número pré-definido de árvores de decisão, cada uma dessas árvores é feita a partir de certas colunas da base de dados, essas colunas são escolhidas randomicamente, ao contrário de uma árvore comum que usa apenas uma árvore com todas as colunas da base de dados. O parâmetro n_estimators recebe o número de árvores que se deseja criar, o parâmetro criterion recebe o valor entropy para que as árvores criadas sejam baseadas no ganho de informação por entropia, já o random_state recebe um valor qualquer para que após cada execução, os valores não se alterem, sendo útil para testes;

################################################################################################################
Orange:

Orange.data.Table((caminho do arquivo csv)) -> Lê um arquivo csv;

(objeto base de dados Orange).domain -> Informa os nomes das colunas da base de dados;

(objeto CN2Learner)(base de dados Orange) -> Cria um objeto que detem as regras de decisão com base no algoritmo CN2Learner;

(objeto regras CN2Learner).rule_list -> Informa através de uma iteração a lista de regras criada pelo algoritmo CN2Learner;

(objeto regras CN2Learner)([[dados previsores treinamento]]) -> Recebe os dados previsores de treinamento e a partir das regras definidas, informa a qual classe os dados informados pertencem. O valor que ele retorna é referente ao índice que representa a sua classe;

(objeto base de dados Orange).domain.class_var.values -> Retorna uma tupla que informa as classes existentes na base de dados, cada opção de classe pode ser representada pelo índice da tupla retornada;

Orange.evaluation.testing.sample(base de dados em formato Orange, n = (percentual da base de dados que será dedicado ao teste)) -> Divide a base de dados em duas listas, a primeira lista, com índice 0, representa os dados de teste, já a segunda lista, a de índice 1, representa os dados de treinamento;

Orange.evaluation.testing.TestOnTestData(base de treinamento, base de teste, [lambda testdata: variável com as regras]) -> Realiza as predições com base nas regras estabelecidas e com base nos dados recebidos;

Orange.evaluation.CA(predições) -> Recebe as predições e informa o percentual de acerto do algoritmo;

(registro da base de dados Orange).get_class() -> Informa a qual classe pertence um determinado registro da base de dados Orange;

################################################################################################################

Counter(item) -> Retorna um dicionário onde cada chave é um dos valores que aparecem e seu valor é a quantidade de vezes que o valor aparece;

KNeighborsClassifier(n_neighbors = (número de vizinhos que você quer comparar (5 é o padrão)), metric='minkowski', p=2) -> O kNN é um algoritmo que classifica novos registros com base na proximidade que esses registros possuem se comparados com a base de treinamento. Ele recebe o número de vizinhos que se deseja abranger, a métrica minkowski e o p=2 que informa que o método utilizado para calcular a distância será baseado no cálculo euclidiano; 

np.delete(variável com base de dados, [índices dos valores que se deseja apagar], axis=(0=linha/1=coluna)) -> Apaga linhas ou colunas de uma base de dados, para isso, ele recebe uma base de dados, os índices que se deseja apagar e, por fim, o atributo axis que quanto recebe 0 se refere as linhas, ou seja, acabando por fazer ele apagar as linhas, e quando recebe 1 se refere as colunas, fazendo ele apagar as colunas, ambos referentes aos respectivos índices. Retornando, por fim, uma lista sem os valores que foram retirados;

LogisticRegression(random_state = (qualquer número), max_iter = (número de vezes que se deseja rodar o algoritmo para encontrar os valores de B)) -> Constroi um objeto de Regressão Logistíca. Pode receber o parâmetro max_iter, que diz quantas vezes o algoritmo vai rodar para consiguir definir os valores de B;

(objeto LogisticRegression).intercept_ -> Retorna o valor de B0;

(objeto LogisticRegression).coef_ -> Retorna uma lista contendo os valores de B1, B2, B3 ... BN;

SVC(kernel=(kernel que será usado), random_state=(qualquer número), C=(valor de punição)) -> Instancia um objeto SVM, para isso, ele recebe como parâmetro o kernel que representa o algoritmo que será utilizado para converter medidas não-linearmente separáveis para medidas linearmente separáveis. Além disso, ele também recebe como parametro o C, que diz respeito ao valor de punição, quanto maior esse valor maior a eficácia desse algoritmo, porém, é necessário ter cautela, números muito grandes exigem grandes capacidades de processamento, além de que chega um ponto em que é impossível melhorar a eficácia do algoritmo, mesmo aumentando o número. Então sempre é bom realizar vários testes, tanto referentes ao kernel quanto referentes ao valor do C, antes de realizar uma implementação;

MLPClassifier(max_iter, tol, hidden_layer_sizes, solver, verbose, activation, etc) -> O Cria um objeto do tipo rede neural multicamada. O parâmetro max_iter é referente ao número máximo de iterações que o algoritmo realizará para encontrar o ajuste dos pesos. O parâmetro activation é referente a qual função de ativação será utilizada, por padrão usa-se a função relu, que retorna 0 para qualquer valor menor ou igual a zero e o próprio número para os números maiores que zero. Já o parámetro solver é responsável pelo ajuste dos pesos, por padrão ele vem como "adam", este que demonstra grande eficiência em bases de dados grandes. Já o prâmetro tol é referente à tolerância máxima que o algoritmo encontra entre uma iteração e outra, caso o valor encontrado durante as iterações seja menor que o valor de tolerância o algoritmo para de fazer reajustes independente do número máximo de iterações definido. O hidden_layer_sizes é o parâmetro que define o número de neurônios de cada camada oculta, caso haja mais de uma camada oculta, deve se passar o número de neurônios de cada camada a partir de uma tupla. O parâmetro verbose é útil para descobrir qual o percentual de melhora do algoritmo após cada iteração. Entre outros parâmetros;

np.concatenate((variável 1, variável 2, ..., variável n), axis = 0) -> Concatena duas ou mais variáveis em uma única, recebendo como parâmetros as variáveis e o axis que quando igual a 0 diz respeito as linhas e quando igual a 1 diz respeito as colunas. Nesse caso, o parâmetro axis deve ser atribuido como igual a 0;

GrindSearchCV(estimator = tipo de algoritmo de machine learning, param_grid = dicionário com a chave referente ao nome do parâmetro e o valor é uma lista contendo os diferentes valores que se deseja testar) -> Instancia um objeto GrindSearchCV que testa a melhor combinação de parâmetros visando, assim, encontrar o maior percentual de acerto;

(objeto GrindSearchCV).best_param_ -> Após realizar o treinamento, informa a melhor combinação de parâmetros com base nos parâmetros informados;

(objeto GrindSearchCV).best_score_ -> Após realizar o treinamento, informa, com base na melhor combinação dos parâmetros informados, o melhor percentual de acerto;

KFold(n_splits = (número de divisões (10 por convenção)), shuffle = True, random_state = i) -> O KFold torna possível dividir a base de dados em treinamento e testes de n_splits maneiras diferentes, cada uma dessas maneiras possui um resultado de predição diferente. O parâmetro shuffle deve ser atribuido como True para que seja possível misturar os dados. Além disso, como ele normalmente, para realizar testes de validação, é usando em um for de range 30, usá-se o parâmetro random_state como sendo igual ao valor daquela iteração, ou seja, igual ao valor de i;

cross_val_score(objeto de alguma classe de algoritmo de Machine Learning, elementos previsores (x), classe (y), cv = objeto KFold) -> Calcula o percentual de acerto do algoritmo para cada diferente divisão da base de dados realizada pelo objeto KFold que foi passado como parâmetro;

pd.DataFrame(dicionário com o nome de coluna desejado como chave e os dados como valor) -> Cria um objeto DataFrame, tornando possível manipular com o Pandas os dados ali presentes;

(objeto DataFrame).var() -> Informa a variância de cada chave (ou coluna) da base de dados;

(objeto DataFrame).std() -> Informa o desvio padrão de cada chave (ou coluna) da base de dados;

shapiro(lista com as médias do percentual de acerto dos algoritmos de predição) -> Retorna, em sua primeira coluna o valor statistic e em sua segunda coluna o valor de p;

f_oneway(listas de algoritmos de Machine Learning contendo, cada um, uma lista com valores dos resultados obtidos) -> Fornece, como segundo valor, o resultado de p, assim é possível realizar o teste ANOVA para desobrir se entre esses valores existe diferença significativa;

MultiComparison(coluna com os valores concatenados dos resultados, coluna com o nome do algoritmo que tornou possível obter cada um dos diferentes dados) -> Cria um objeto de comparação multipla;

(objeto MultiComparison).tukeyhsd() -> Realiza o teste de Tukey, ou seja, informa se entre cada um dos algoritmos existe diferença significativa, caso exista, normalmente, o algoritmo que fornecerá melhores valores e deve ser utilizado é aquele que possui uma média maior dos resultados;

(objeto MultiComparison).plot_simultaneous() -> Cria um gráfica com os dados obtidos entre a comparação dos algoritmos, mostrando, em ordem crescente, qual deles o resultado está mais à frente;
